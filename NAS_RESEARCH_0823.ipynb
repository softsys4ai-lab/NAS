{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha,got it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why don't we start the project NAS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need to find out how do the first paper build the Architecture and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------- Get familiar with the code in autokeras-----------------------------#\n",
    "# ----------------classifier.py\n",
    "# init->fit->evaluate\n",
    "\n",
    "#fit->\n",
    "#    # Transform x,y_train.\n",
    "#    # Create the searcher and save on disk  \n",
    "#    # Divide training data into training and testing data\n",
    "#    # LOOP\n",
    "       $$$run_searcher_once(train_data, test_data, self.path)$$$\n",
    "         if len(self.load_searcher().history) >= Constant.MAX_MODEL_NUM#1000:\n",
    "                break\n",
    "#    #LOOP END\n",
    "#\n",
    "#\n",
    "#         # \n",
    "#         ###run_searcher_once()->autokeras.utils.pickle_from_file() CALL searcher.search()\n",
    "#             input_shape = x_train.shape[1:]\n",
    "            n_classes = self.y_encoder.n_classes #An integer, the number of classes\n",
    "            self.searcher_args['n_classes'] = n_classes #An integer, the number of classes\n",
    "            self.searcher_args['input_shape'] = input_shape #A tuple. e.g. (28, 28, 1)\n",
    "            self.searcher_args['path'] = self.path#The path to the directory to save the searcher\n",
    "            self.searcher_args['verbose'] = self.verbose#A boolean. Whether to output the intermediate information to stdout.\n",
    "            $$$$  searcher = BayesianSearcher(**self.searcher_args) $$$$\n",
    "            self.save_searcher(searcher)\n",
    "            self.searcher = True\n",
    "#-----------------Search.py\n",
    "\n",
    "#           ->__init__()\n",
    "                 default_model_len=Constant.MODEL_LEN,#3\n",
    "                 default_model_width=Constant.MODEL_WIDTH,#64\n",
    "                 beta=Constant.BETA,#0.576\n",
    "                 kernel_lambda=Constant.KERNEL_LAMBDA#1.0\n",
    "                if trainer_args is None:\n",
    "                    trainer_args = {}\n",
    "                if 'max_iter_num' not in self.trainer_args:\n",
    "                self.trainer_args['max_iter_num'] = Constant.SEARCH_MAX_ITER#200\n",
    "                self.gpr = IncrementalGaussianProcess(kernel_lambda)\n",
    "                self.search_tree = SearchTree()\n",
    "                self.training_queue = []\n",
    "                self.x_queue = []\n",
    "                self.y_queue = []\n",
    "                self.beta = beta\n",
    "                if t_min is None:\n",
    "                    t_min = Constant.T_MIN#0.0001\n",
    "                self.t_min = t_min\n",
    "#           ->search()\n",
    "#             ->init_search()  start\n",
    "#------------------generator.py\n",
    "                graph = DefaultClassifierGenerator(self.n_classes,\n",
    "                self.input_shape).generate(self.default_model_len,\n",
    "                                          self.default_model_width)\n",
    "                    $$$$$generate$$$$$\n",
    "                        model_width=Constant.MODEL_WIDTH#64):\n",
    "                        pooling_len = int(model_len#3 / 4)\n",
    "                        graph = Graph(self.input_shape, False)\n",
    "#------------------Graph.py                                          \n",
    "                          $$$$$$Graph.__init__start$$$$$$\n",
    "                                    self.weighted = weighted#A boolean of whether the weights and biases in the neural network should be included in the graph.\n",
    "                                    self.node_list = []#A list of integers, the indices of the list are the identifiers.\n",
    "                                    self.layer_list = []#A list of stub layers, the indices of the list are the identifiers.\n",
    "                                    # node id start with 0\n",
    "                                    self.node_to_id = {}#A dict instance mapping from node integers to their identifiers.\n",
    "                                    self.layer_to_id = {}# A dict instance mapping from stub layers to their identifiers.\n",
    "                                    self.layer_id_to_input_node_ids = {}#A dict instance mapping from layer identifiers to their input nodes identifiers.\n",
    "                                    self.layer_id_to_output_node_ids = {}\n",
    "                                    self.adj_list = {}#A two dimensional list. The adjacency list of the graph. The first dimension is identified by tensor identifiers. In each edge list, the elements are two-element tuples of (tensor identifier, layer identifier).\n",
    "                                    self.reverse_adj_list = {}#A reverse adjacent list in the same format as adj_list.\n",
    "                                    self.operation_history = []#A list saving all the network morphism operations.\n",
    "\n",
    "                                    self.vis = None#A dictionary of temporary storage for whether an local operation has been done during the network morphism.\n",
    "                                    self._add_node(Node(input_shape))\n",
    "                          $$$$$$Graph.__init__end$$$$$$\n",
    "                        temp_input_channel = self.input_shape[-1]\n",
    "                        output_node_id = 0\n",
    "                        for i in range(model_len):\n",
    "                            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n",
    "                            output_node_id = graph.add_layer(StubConv(temp_input_channel, model_width, kernel_size=3), output_node_id)\n",
    "                            output_node_id = graph.add_layer(StubBatchNormalization(model_width), output_node_id)\n",
    "                            output_node_id = graph.add_layer(StubDropout(Constant.CONV_DROPOUT_RATE), output_node_id)\n",
    "                            temp_input_channel = model_width\n",
    "                            if pooling_len == 0 or ((i + 1) % pooling_len == 0 and i != model_len - 1):\n",
    "                                output_node_id = graph.add_layer(StubPooling(), output_node_id)\n",
    "\n",
    "                        output_node_id = graph.add_layer(StubFlatten(), output_node_id)\n",
    "                        output_node_id = graph.add_layer(StubDense(graph.node_list[output_node_id].shape[0], self.n_classes),\n",
    "                                                         output_node_id)\n",
    "                        graph.add_layer(StubSoftmax(), output_node_id)\n",
    "                        return graph\n",
    "                model_id = self.model_count\n",
    "                self.model_count += 1\n",
    "                self.training_queue.append((graph, -1, model_id))\n",
    "                self.descriptors.append(graph.extract_descriptor())\n",
    "                for child_graph in default_transform(graph):\n",
    "                    child_id = self.model_count\n",
    "                    self.model_count += 1\n",
    "                    self.training_queue.append((child_graph, model_id, child_id))\n",
    "                    self.descriptors.append(child_graph.extract_descriptor())\n",
    "                if self.verbose:\n",
    "                    print('Initialization finished.') \n",
    "#            ->init_search() end\n",
    "#            # Start the new process for training.\n",
    "            graph, father_id, model_id = self.training_queue.pop(0)\n",
    "            if self.verbose:\n",
    "                print('Training model ', model_id)\n",
    "                multiprocessing.set_start_method('spawn', force=True)\n",
    "                pool = multiprocessing.Pool(1)\n",
    "                $$$$$$$train_results = pool.map_async(train, [(graph, train_data, test_data, self.trainer_args,\n",
    "                                                        os.path.join(self.path, str(model_id) + '.png'), self.verbose)])                        \n",
    "#            # Do the search in current thread.\n",
    "            if not self.training_queue:\n",
    "                new_graph, new_father_id = self.maximize_acq()# produce the next graph based on the\n",
    "                new_model_id = self.model_count\n",
    "                self.model_count += 1\n",
    "                self.training_queue.append((new_graph, new_father_id, new_model_id))\n",
    "                descriptor = new_graph.extract_descriptor()\n",
    "                self.descriptors.append(new_graph.extract_descriptor())\n",
    "\n",
    "            accuracy, loss, graph = train_results.get()[0]\n",
    "            pool.terminate()\n",
    "            pool.join()\n",
    "            self.add_model(accuracy, loss, graph, model_id)\n",
    "            self.search_tree.add_child(father_id, model_id)\n",
    "            self.gpr.fit(self.x_queue, self.y_queue)\n",
    "            self.x_queue = []\n",
    "            self.y_queue = []\n",
    "\n",
    "            pickle_to_file(self, os.path.join(self.path, 'searcher'))\n",
    "            self.export_json(os.path.join(self.path, 'history.json'))\n",
    "\n",
    "\n",
    "# ->DefaultClassifierGenerator->Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minst.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Minst dataset using autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing search.\n",
      "Plotting model init  1111111111\n",
      "Initialization finished.\n",
      "before the new proc\n",
      "Plotting model  0\n",
      "Training model  0\n",
      "Saving model.\n",
      "Model ID: 0\n",
      "Loss: tensor(1.1282)\n",
      "Accuracy 99.18\n",
      "before the new proc\n",
      "Plotting model  1\n",
      "Training model  1\n",
      "Father ID:  0\n",
      "[('to_wider_model', 6, 64)]\n",
      "Saving model.\n",
      "Model ID: 1\n",
      "Loss: tensor(0.9911)\n",
      "Accuracy 99.396\n",
      "before the new proc\n",
      "Plotting model  2\n",
      "Training model  2\n",
      "Father ID:  1\n",
      "[('to_wider_model', 11, 64)]\n",
      "Saving model.\n",
      "Model ID: 2\n",
      "Loss: tensor(1.2862)\n",
      "Accuracy 99.212\n",
      "before the new proc\n",
      "Plotting model  3\n",
      "Training model  3\n",
      "Father ID:  2\n",
      "[('to_wider_model', 6, 128)]\n",
      "Saving model.\n",
      "Model ID: 3\n",
      "Loss: tensor(1.0435)\n",
      "Accuracy 99.41599999999998\n",
      "...............................................\n",
      "Epoch 1: loss 2.6209282875061035, accuracy 98.87\n",
      "...............................................\n",
      "Epoch 2: loss 2.1810882091522217, accuracy 99.15\n",
      "...............................................\n",
      "Epoch 3: loss 1.8129054307937622, accuracy 99.14\n",
      "...............................................\n",
      "Epoch 4: loss 1.623627781867981, accuracy 99.3\n",
      "...............................................\n",
      "Epoch 5: loss 1.3970390558242798, accuracy 99.37\n",
      "...............................................\n",
      "Epoch 6: loss 1.6224827766418457, accuracy 99.27\n",
      "...............................................\n",
      "Epoch 7: loss 2.058013916015625, accuracy 99.19\n",
      "...............................................\n",
      "Epoch 8: loss 1.4217311143875122, accuracy 99.45\n",
      "...............................................\n",
      "Epoch 9: loss 1.7769726514816284, accuracy 99.34\n",
      "...............................................\n",
      "Epoch 10: loss 2.125208854675293, accuracy 99.14\n",
      "...............................................\n",
      "Epoch 11: loss 1.4043781757354736, accuracy 99.42\n",
      "...............................................\n",
      "Epoch 12: loss 1.4472912549972534, accuracy 99.47\n",
      "No loss decrease after 5 epochs\n",
      "99.46000000000001\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from autokeras.classifier import ImageClassifier\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "    clf = ImageClassifier(verbose=True, augment=False)\n",
    "    clf.fit(x_train, y_train, time_limit=9 * 60 * 60)\n",
    "    clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)\n",
    "    y = clf.evaluate(x_test, y_test)\n",
    "    print(y * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check keras status\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# check new plot func \n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "\n",
    "#    plot_model(model,\n",
    "               to_file='model.png',\n",
    "               show_shapes=False,\n",
    "               show_layer_names=True,\n",
    "               rankdir='TB')\n",
    "#plot(model, to_file='model.png')                    \n",
    "    \"\"\"Converts a Keras model to dot format and save to a file.\n",
    "\n",
    "    # Arguments\n",
    "        model: A Keras model instance\n",
    "        to_file: File name of the plot image.\n",
    "        show_shapes: whether to display shape information.\n",
    "        show_layer_names: whether to display layer names.\n",
    "        rankdir: `rankdir` argument passed to PyDot,\n",
    "            a string specifying the format of the plot:\n",
    "            'TB' creates a vertical plot;\n",
    "            'LR' creates a horizontal plot.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping from code to paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqusition function\n",
    "    def maximize_acq(self):\n",
    "        model_ids = self.search_tree.adj_list.keys()\n",
    "        target_graph = None\n",
    "        father_id = None\n",
    "        descriptors = deepcopy(self.descriptors)\n",
    "\n",
    "        # Initialize the priority queue.\n",
    "        pq = PriorityQueue()\n",
    "        temp_list = []\n",
    "        for model_id in model_ids:\n",
    "            accuracy = self.get_accuracy_by_id(model_id)\n",
    "            temp_list.append((accuracy, model_id))\n",
    "        temp_list = sorted(temp_list)\n",
    "        for accuracy, model_id in temp_list:\n",
    "            graph = self.load_model_by_id(model_id)\n",
    "            graph.clear_operation_history()\n",
    "            pq.put(Elem(accuracy, model_id, graph))\n",
    "\n",
    "        t = 1.0\n",
    "        t_min = self.t_min\n",
    "        alpha = 0.9\n",
    "        max_acq = -1\n",
    "        while not pq.empty() and t > t_min:\n",
    "            elem = pq.get()\n",
    "            temp_exp = min((elem.accuracy - max_acq) / t, 709.0)\n",
    "            ap = math.exp(temp_exp)\n",
    "            if ap > random.uniform(0, 1):\n",
    "                graphs = transform(elem.graph)\n",
    "\n",
    "                for temp_graph in graphs:\n",
    "                    if contain(descriptors, temp_graph.extract_descriptor()):\n",
    "                        continue\n",
    "\n",
    "                    temp_acq_value = self.acq(temp_graph)\n",
    "                    pq.put(Elem(temp_acq_value, elem.father_id, temp_graph))\n",
    "                    descriptors.append(temp_graph.extract_descriptor())\n",
    "                    if temp_acq_value > max_acq:\n",
    "                        max_acq = temp_acq_value\n",
    "                        father_id = elem.father_id\n",
    "                        target_graph = temp_graph\n",
    "            t *= alpha\n",
    "\n",
    "        nm_graph = self.load_model_by_id(father_id)\n",
    "        if self.verbose:\n",
    "            print('Father ID: ', father_id)\n",
    "            print(target_graph.operation_history)\n",
    "        for args in target_graph.operation_history:\n",
    "            getattr(nm_graph, args[0])(*list(args[1:]))\n",
    "        return nm_graph, father_id\n",
    "\n",
    "    def acq(self, graph):\n",
    "        mean, std = self.gpr.predict(np.array([graph.extract_descriptor()]))\n",
    "        return mean + self.beta * std\n",
    "#kernel\n",
    "def layer_distance(a, b):\n",
    "    return abs(a - b) * 1.0 / max(a, b)\n",
    "\n",
    "\n",
    "def layers_distance(list_a, list_b):\n",
    "    len_a = len(list_a)\n",
    "    len_b = len(list_b)\n",
    "    f = np.zeros((len_a + 1, len_b + 1))\n",
    "    f[-1][-1] = 0\n",
    "    for i in range(-1, len_a):\n",
    "        f[i][-1] = i + 1\n",
    "    for j in range(-1, len_b):\n",
    "        f[-1][j] = j + 1\n",
    "    for i in range(len_a):\n",
    "        for j in range(len_b):\n",
    "            f[i][j] = min(f[i][j - 1] + 1, f[i - 1][j] + 1, f[i - 1][j - 1] + layer_distance(list_a[i], list_b[j]))\n",
    "    return f[len_a - 1][len_b - 1]\n",
    "\n",
    "\n",
    "def skip_connection_distance(a, b):\n",
    "    if a[2] != b[2]:\n",
    "        return 1.0\n",
    "    len_a = abs(a[1] - a[0])\n",
    "    len_b = abs(b[1] - b[0])\n",
    "    return (abs(a[0] - b[0]) + abs(len_a - len_b)) / (max(a[0], b[0]) + max(len_a, len_b))\n",
    "\n",
    "\n",
    "def skip_connections_distance(list_a, list_b):\n",
    "    distance_matrix = np.zeros((len(list_a), len(list_b)))\n",
    "    for i, a in enumerate(list_a):\n",
    "        for j, b in enumerate(list_b):\n",
    "            distance_matrix[i][j] = skip_connection_distance(a, b)\n",
    "    return distance_matrix[linear_sum_assignment(distance_matrix)].sum() + abs(len(list_a) - len(list_b))\n",
    "\n",
    "\n",
    "def edit_distance(x, y, kernel_lambda):\n",
    "    ret = 0\n",
    "    ret += layers_distance(x.conv_widths, y.conv_widths)\n",
    "    ret += layers_distance(x.dense_widths, y.dense_widths)\n",
    "    ret += kernel_lambda * skip_connections_distance(x.skip_connections, y.skip_connections)\n",
    "    return ret\n",
    "\n",
    "\n",
    "    def incremental_fit(self, train_x, train_y):\n",
    "        if not self._first_fitted:\n",
    "            raise ValueError(\"The first_fit function needs to be called first.\")\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "        # Incrementally compute K\n",
    "        up_right_k = self.edit_distance_matrix(self.kernel_lambda, self._x, train_x)  # Shape (len(X_train_), len(train_x))\n",
    "        down_left_k = np.transpose(up_right_k)\n",
    "        down_right_k = self.edit_distance_matrix(self.kernel_lambda, train_x)\n",
    "        up_k = np.concatenate((self._distance_matrix, up_right_k), axis=1)\n",
    "        down_k = np.concatenate((down_left_k, down_right_k), axis=1)\n",
    "        self._distance_matrix = np.concatenate((up_k, down_k), axis=0)\n",
    "        self._distance_matrix = bourgain_embedding_matrix(self._distance_matrix)\n",
    "        self._k_matrix = 1.0 / np.exp(self._distance_matrix)\n",
    "        diag = np.diag_indices_from(self._k_matrix)\n",
    "        diag = (diag[0][-len(train_x):], diag[1][-len(train_x):])\n",
    "        self._k_matrix[diag] += self.alpha\n",
    "\n",
    "        self._x = np.concatenate((self._x, train_x), axis=0)\n",
    "        self._y = np.concatenate((self._y, train_y), axis=0)\n",
    "\n",
    "        self._l_matrix = cholesky(self._k_matrix, lower=True)  # Line 2\n",
    "\n",
    "        self._alpha_vector = cho_solve((self._l_matrix, True), self._y)  # Line 3\n",
    "\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def first_fitted(self):\n",
    "        return self._first_fitted\n",
    "\n",
    "    def first_fit(self, train_x, train_y):\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "        self._x = np.copy(train_x)\n",
    "        self._y = np.copy(train_y)\n",
    "\n",
    "        self._distance_matrix = self.edit_distance_matrix(self.kernel_lambda, self._x)\n",
    "        self._distance_matrix = bourgain_embedding_matrix(self._distance_matrix)\n",
    "        self._k_matrix = 1.0 / np.exp(self._distance_matrix)\n",
    "        self._k_matrix[np.diag_indices_from(self._k_matrix)] += self.alpha\n",
    "\n",
    "        self._l_matrix = cholesky(self._k_matrix, lower=True)  # Line 2\n",
    "\n",
    "        self._alpha_vector = cho_solve((self._l_matrix, True), self._y)  # Line 3\n",
    "\n",
    "        self._first_fitted = True\n",
    "        return self\n",
    "def edit_distance_matrix(kernel_lambda, train_x, train_y=None):\n",
    "    if train_y is None:\n",
    "        ret = np.zeros((train_x.shape[0], train_x.shape[0]))\n",
    "        for x_index, x in enumerate(train_x):\n",
    "            for y_index, y in enumerate(train_x):\n",
    "                if x_index == y_index:\n",
    "                    ret[x_index][y_index] = 0\n",
    "                elif x_index < y_index:\n",
    "                    ret[x_index][y_index] = edit_distance(x, y, kernel_lambda)\n",
    "                else:\n",
    "                    ret[x_index][y_index] = ret[y_index][x_index]\n",
    "        return ret\n",
    "    ret = np.zeros((train_x.shape[0], train_y.shape[0]))\n",
    "    for x_index, x in enumerate(train_x):\n",
    "        for y_index, y in enumerate(train_y):\n",
    "            ret[x_index][y_index] = edit_distance(x, y, kernel_lambda)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def vector_distance(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "\n",
    "def bourgain_embedding_matrix(distance_matrix):\n",
    "    distance_matrix = np.array(distance_matrix)\n",
    "    n = len(distance_matrix)\n",
    "    if n == 1:\n",
    "        return distance_matrix\n",
    "    np.random.seed(123)\n",
    "    distort_elements = []\n",
    "    r = range(n)\n",
    "    k = int(math.ceil(math.log(n) / math.log(2) - 1))\n",
    "    t = int(math.ceil(math.log(n)))\n",
    "    counter = 0\n",
    "    for i in range(0, k + 1):\n",
    "        for t in range(t):\n",
    "            s = np.random.choice(r, 2 ** i)\n",
    "            for j in r:\n",
    "                d = min([distance_matrix[j][s] for s in s])\n",
    "                counter += len(s)\n",
    "                if i == 0 and t == 0:\n",
    "                    distort_elements.append([d])\n",
    "                else:\n",
    "                    distort_elements[j].append(d)\n",
    "    distort_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distort_matrix[i][j] = distort_matrix[j][i] = vector_distance(distort_elements[i], distort_elements[j])\n",
    "    return np.array(distort_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morph operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph morphism\n",
    "def transform(graph):\n",
    "    graphs = []\n",
    "    for i in range(Constant.N_NEIGHBOURS):\n",
    "        a = randrange(3)\n",
    "        if a == 0:\n",
    "            graphs.append(to_deeper_graph(deepcopy(graph)))\n",
    "        elif a == 1:\n",
    "            graphs.append(to_wider_graph(deepcopy(graph)))\n",
    "        elif a == 2:\n",
    "            graphs.append(to_skip_connection_graph(deepcopy(graph)))\n",
    "    return list(filter(lambda x: legal_graph(x), graphs))\n",
    "\n",
    "\n",
    "def to_wider_graph(graph):\n",
    "    weighted_layer_ids = graph.wide_layer_ids()\n",
    "    if len(weighted_layer_ids) <= 1:\n",
    "        target_id = weighted_layer_ids[0]\n",
    "    else:\n",
    "        target_id = weighted_layer_ids[randint(0, len(weighted_layer_ids) - 1)]\n",
    "\n",
    "    if is_layer(graph.layer_list[target_id], 'Conv'):\n",
    "        n_add = graph.layer_list[target_id].filters\n",
    "    else:\n",
    "        n_add = graph.layer_list[target_id].units\n",
    "\n",
    "    graph.to_wider_model(target_id, n_add)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def to_skip_connection_graph(graph):\n",
    "    # The last conv layer cannot be widen since wider operator cannot be done over the two sides of flatten.\n",
    "    weighted_layer_ids = graph.skip_connection_layer_ids()\n",
    "    index_a = randint(0, len(weighted_layer_ids) - 1)\n",
    "    index_b = randint(0, len(weighted_layer_ids) - 1)\n",
    "    if index_a == index_b:\n",
    "        if index_b == 0:\n",
    "            index_a = index_b + 1\n",
    "        else:\n",
    "            index_a = index_b - 1\n",
    "    if index_a > index_b:\n",
    "        index_a, index_b = index_b, index_a\n",
    "    a_id = weighted_layer_ids[index_a]\n",
    "    b_id = weighted_layer_ids[index_b]\n",
    "    if graph.layer_list[a_id].filters == graph.layer_list[b_id].filters:\n",
    "        graph.to_add_skip_model(a_id, b_id)\n",
    "    else:\n",
    "        graph.to_concat_skip_model(a_id, b_id)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def to_deeper_graph(graph):\n",
    "    weighted_layer_ids = graph.deep_layer_ids()\n",
    "    target_id = weighted_layer_ids[randint(0, len(weighted_layer_ids) - 1)]\n",
    "    if is_layer(graph.layer_list[target_id], 'Conv'):\n",
    "        graph.to_conv_deeper_model(target_id, randint(1, 2) * 2 + 1)\n",
    "    else:\n",
    "        graph.to_dense_deeper_model(target_id)\n",
    "    return graph\n",
    "\n",
    "#graph func\n",
    "    def to_dense_deeper_model(self, target_id):\n",
    "        \"\"\"Insert a dense layer after the target layer.\n",
    "\n",
    "        Args:\n",
    "            target_id: The ID of a dense layer.\n",
    "        \"\"\"\n",
    "        self.operation_history.append(('to_dense_deeper_model', target_id))\n",
    "        target = self.layer_list[target_id]\n",
    "        new_layers = dense_to_deeper_block(target, self.weighted)\n",
    "        output_id = self._dense_block_end_node(target_id)\n",
    "\n",
    "        self._insert_new_layers(new_layers, output_id)\n",
    "    def to_conv_deeper_model(self, target_id, kernel_size):\n",
    "        \"\"\"Insert a relu-conv-bn block after the target block.\n",
    "\n",
    "        Args:\n",
    "            target_id: A convolutional layer ID. The new block should be inserted after the block.\n",
    "            kernel_size: An integer. The kernel size of the new convolutional layer.\n",
    "        \"\"\"\n",
    "        self.operation_history.append(('to_conv_deeper_model', target_id, kernel_size))\n",
    "        target = self.layer_list[target_id]\n",
    "        new_layers = deeper_conv_block(target, kernel_size, self.weighted)\n",
    "        output_id = self._conv_block_end_node(target_id)\n",
    "\n",
    "        self._insert_new_layers(new_layers, output_id)\n",
    "\n",
    "    def to_wider_model(self, pre_layer_id, n_add):\n",
    "        \"\"\"Widen the last dimension of the output of the pre_layer.\n",
    "\n",
    "        Args:\n",
    "            pre_layer_id: The ID of a convolutional layer or dense layer.\n",
    "            n_add: The number of dimensions to add.\n",
    "        \"\"\"\n",
    "        self.operation_history.append(('to_wider_model', pre_layer_id, n_add))\n",
    "        pre_layer = self.layer_list[pre_layer_id]\n",
    "        output_id = self.layer_id_to_output_node_ids[pre_layer_id][0]\n",
    "        dim = layer_width(pre_layer)\n",
    "        self.vis = {}\n",
    "        self._search(output_id, dim, dim, n_add)\n",
    "        for u in self.topological_order:\n",
    "            for v, layer_id in self.adj_list[u]:\n",
    "                self.node_list[v].shape = self.layer_list[layer_id].output_shape\n",
    "    def to_concat_skip_model(self, start_id, end_id):\n",
    "        \"\"\"Add a weighted add concatenate connection from after start node to end node.\n",
    "\n",
    "        Args:\n",
    "            start_id: The convolutional layer ID, after which to start the skip-connection.\n",
    "            end_id: The convolutional layer ID, after which to end the skip-connection.\n",
    "        \"\"\"\n",
    "        self.operation_history.append(('to_concat_skip_model', start_id, end_id))\n",
    "        conv_block_input_id = self._conv_block_end_node(start_id)\n",
    "        conv_block_input_id = self.adj_list[conv_block_input_id][0][0]\n",
    "\n",
    "        block_last_layer_input_id = self._conv_block_end_node(end_id)\n",
    "\n",
    "        # Add the pooling layer chain.\n",
    "        pooling_layer_list = self._get_pooling_layers(conv_block_input_id, block_last_layer_input_id)\n",
    "        skip_output_id = conv_block_input_id\n",
    "        for index, layer_id in enumerate(pooling_layer_list):\n",
    "            skip_output_id = self.add_layer(deepcopy(self.layer_list[layer_id]), skip_output_id)\n",
    "\n",
    "        block_last_layer_output_id = self.adj_list[block_last_layer_input_id][0][0]\n",
    "        concat_input_node_id = self._add_node(deepcopy(self.node_list[block_last_layer_output_id]))\n",
    "        self._redirect_edge(block_last_layer_input_id, block_last_layer_output_id, concat_input_node_id)\n",
    "\n",
    "        concat_layer = StubConcatenate()\n",
    "        concat_layer.input = [self.node_list[concat_input_node_id], self.node_list[skip_output_id]]\n",
    "        concat_output_node_id = self._add_node(Node(concat_layer.output_shape))\n",
    "        self._add_edge(concat_layer, concat_input_node_id, concat_output_node_id)\n",
    "        self._add_edge(concat_layer, skip_output_id, concat_output_node_id)\n",
    "        concat_layer.output = self.node_list[concat_output_node_id]\n",
    "        self.node_list[concat_output_node_id].shape = concat_layer.output_shape\n",
    "\n",
    "        # Add the concatenate layer.\n",
    "        new_relu_layer = StubReLU()\n",
    "        concat_output_node_id = self.add_layer(new_relu_layer, concat_output_node_id)\n",
    "        new_conv_layer = StubConv(self.layer_list[start_id].filters + self.layer_list[end_id].filters,\n",
    "                                  self.layer_list[end_id].filters, 1)\n",
    "        concat_output_node_id = self.add_layer(new_conv_layer, concat_output_node_id)\n",
    "        new_bn_layer = StubBatchNormalization(self.layer_list[end_id].filters)\n",
    "\n",
    "        self._add_edge(new_bn_layer, concat_output_node_id, block_last_layer_output_id)\n",
    "        new_bn_layer.input = self.node_list[concat_output_node_id]\n",
    "        new_bn_layer.output = self.node_list[block_last_layer_output_id]\n",
    "        self.node_list[block_last_layer_output_id].shape = new_bn_layer.output_shape\n",
    "\n",
    "        if self.weighted:\n",
    "            filters_end = self.layer_list[end_id].filters\n",
    "            filters_start = self.layer_list[start_id].filters\n",
    "            filter_shape = (1,) * (len(self.layer_list[end_id].get_weights()[0].shape) - 2)\n",
    "            weights = np.zeros((filters_end, filters_end) + filter_shape)\n",
    "            for i in range(filters_end):\n",
    "                filter_weight = np.zeros((filters_end,) + filter_shape)\n",
    "                filter_weight[(i, 0, 0)] = 1\n",
    "                weights[i, ...] = filter_weight\n",
    "            weights = np.concatenate((weights,\n",
    "                                      np.zeros((filters_end, filters_start) + filter_shape)), axis=1)\n",
    "            bias = np.zeros(filters_end)\n",
    "            new_conv_layer.set_weights((add_noise(weights, np.array([0, 1])), add_noise(bias, np.array([0, 1]))))\n",
    "\n",
    "            n_filters = filters_end\n",
    "            new_weights = [add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n",
    "                           add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n",
    "                           add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n",
    "                           add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1]))]\n",
    "            new_bn_layer.set_weights(new_weights)    \n",
    "#basic operation\n",
    "    def _insert_new_layers(self, new_layers, output_id):\n",
    "        new_node_id = self._add_node(deepcopy(self.node_list[self.adj_list[output_id][0][0]]))\n",
    "        temp_output_id = new_node_id\n",
    "        for layer in new_layers[:-1]:\n",
    "            temp_output_id = self.add_layer(layer, temp_output_id)\n",
    "\n",
    "        self._add_edge(new_layers[-1], temp_output_id, self.adj_list[output_id][0][0])\n",
    "        new_layers[-1].input = self.node_list[temp_output_id]\n",
    "        new_layers[-1].output = self.node_list[self.adj_list[output_id][0][0]]\n",
    "        self._redirect_edge(output_id, self.adj_list[output_id][0][0], new_node_id)\n",
    "\n",
    "    def _block_end_node(self, layer_id, block_size):\n",
    "        ret = self.layer_id_to_output_node_ids[layer_id][0]\n",
    "        for i in range(block_size - 2):\n",
    "            ret = self.adj_list[ret][0][0]\n",
    "        return ret\n",
    "\n",
    "    def _dense_block_end_node(self, layer_id):\n",
    "        return self._block_end_node(layer_id, Constant.DENSE_BLOCK_DISTANCE)\n",
    "\n",
    "    def _conv_block_end_node(self, layer_id):\n",
    "        \"\"\"Get the input node ID of the last layer in the block by layer ID.\n",
    "            Return the input node ID of the last layer in the convolutional block.\n",
    "\n",
    "        Args:\n",
    "            layer_id: the convolutional layer ID.\n",
    "        \"\"\"\n",
    "        return self._block_end_node(layer_id, Constant.CONV_BLOCK_DISTANCE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
